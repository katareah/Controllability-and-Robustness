{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for training language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torchinfo\n",
    "from contextlib import nullcontext\n",
    "from  tqdm import tqdm, trange\n",
    "import argparse\n",
    "\n",
    "\n",
    "import sys; sys.path += ['..']\n",
    "from language_models import TransformerLM, configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    batch_size=128, n_epochs=20, log_on_step=True,\n",
    "    wandb_project=\"controllability-robustness-test\", run_name='hello', \n",
    "    n_layers=1, n_heads=4, d_model=128, dff=128*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region some configuration\n",
    "device = 'cuada'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "# dtype = 'float32'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "# wandb logging\n",
    "wandb_log = False\n",
    "wandb_project = args.wandb_project\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and vocab\n",
    "data_path = '../name_pairs_dataset/full-sentences.txt'\n",
    "with open(data_path) as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "with open('../name_pairs_dataset/first-names.txt') as f:\n",
    "    first_names = f.read().splitlines()\n",
    "\n",
    "with open('../name_pairs_dataset/last-names.txt') as f:\n",
    "    last_names = f.read().splitlines()\n",
    "\n",
    "# create vocab map\n",
    "vocab = first_names + last_names\n",
    "vocab = [x.lower() for x in vocab]\n",
    "vocab += ['has', 'last', 'name', '[UNK]']\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "vocab_map = {vocab: i for i, vocab in enumerate(vocab)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print('vocab size: ', vocab_size) # vocab size is very large; especially for a task this simple\n",
    "\n",
    "# tokenize and map to integer IDs\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "tokenized_data = [sentence.strip().lower().split(' ') for sentence in tqdm(data)]\n",
    "integer_ids = [torch.tensor([vocab_map.get(token, vocab_map['[UNK]']) for token in tokens]) for tokens in tqdm(tokenized_data)]\n",
    "integer_ids = torch.tensor(np.array(integer_ids))\n",
    "\n",
    "# create dataset and dataloader\n",
    "x = integer_ids[:, :-1]\n",
    "y = integer_ids[:, 1:]\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "train_dataset = dataset[:1000] # NOTE: smaller dataset for testing\n",
    "val_dataset = dataset[1000:2000] # NOTE: smaller dataset for testing\n",
    "train_dl = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "seq_len = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dummy data for testing\n",
    "# vocab_size = 100\n",
    "# n_samples = 1000\n",
    "# seq_len = 16\n",
    "\n",
    "# tokens = torch.randint(0, vocab_size, (n_samples, seq_len+1), dtype=torch.long)\n",
    "# x = tokens[:, :-1]\n",
    "# y = tokens[:, 1:]\n",
    "\n",
    "# dataset = torch.utils.data.TensorDataset(x, y)\n",
    "# train_dl = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "# val_dl = train_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "class LitLanguageModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True, on_step=args.log_on_step, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y)\n",
    "\n",
    "        self.log(f\"val_loss\", loss, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "        self.log(f'val_perplexity', perplexity, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y, z)\n",
    "\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y)\n",
    "\n",
    "        self.log(f\"test_loss\", loss, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "        self.log(f'test_perplexity', perplexity, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # NOTE: i chose these arbitrarily; may need to be tuned\n",
    "        optimizer = configure_optimizers(self.model, weight_decay=0.0, learning_rate=1e-3, betas=(0.9, 0.999), device_type='cuda')\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = args.d_model # 128\n",
    "dff = args.dff # 256\n",
    "n_layers = args.n_layers\n",
    "n_heads = args.n_heads\n",
    "dropout_rate = 0.1\n",
    "activation = 'gelu'\n",
    "norm_first = True\n",
    "\n",
    "\n",
    "model_args = dict(\n",
    "    vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_heads=n_heads, dff=dff,\n",
    "    dropout_rate=dropout_rate, activation=activation, norm_first=norm_first, max_block_size=seq_len)\n",
    "model = TransformerLM(**model_args)#.to(device)\n",
    "torchinfo.summary(model, row_settings=[\"depth\", \"var_names\"], col_names=[\"num_params\", \"params_percent\", \"trainable\"], depth=3, col_width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitLanguageModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = args.n_epochs\n",
    "\n",
    "# run = wandb.init(project=wandb_project, group=group_name, name=run_name,\n",
    "    # config={'group': group_name, **model_args})\n",
    "\n",
    "# wandb_logger = WandbLogger(experiment=run, log_model=False) # name=run_name, project=wandb_project,\n",
    "# wandb_logger.watch(model, log_graph=False)\n",
    "wandb_logger = None\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.TQDMProgressBar(refresh_rate=50)\n",
    "    # L.pytorch.callbacks.RichProgressBar()\n",
    "]\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=n_epochs, enable_checkpointing=False, enable_model_summary=True, #precision='64-true',\n",
    "    callbacks=callbacks, enable_progress_bar=True, check_val_every_n_epoch=1, # limit_train_batches=200, limit_val_batches=200,\n",
    "    logger=False\n",
    "    )\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstract_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
