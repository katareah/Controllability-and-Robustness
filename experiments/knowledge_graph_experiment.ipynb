{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Controllability and Robustness in Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First \"pretrain\" on pure facts (context length/number of context examples 0) \n",
    "- Then \"finetune\" on 1/3 facts, 1/3 counterfactual, 1/3 robustness (context length ranges from 1 to 3 randomly)\n",
    "- Test on counterfactual w/ context length 1,2,3, robustness w/ context length 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torchinfo\n",
    "from contextlib import nullcontext\n",
    "from  tqdm import tqdm, trange\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import sys; sys.path += ['..']\n",
    "from language_models import TransformerLM, configure_optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    batch_size=32, n_epochs=40, log_on_step=True,\n",
    "    wandb_project=\"controllability-robustness-test\", run_name='hello', \n",
    "    n_layers=2, n_heads=4, d_model=128, dff=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region some configuration\n",
    "device = 'cuada'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "# dtype = 'float32'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "# wandb logging\n",
    "wandb_log = True\n",
    "wandb_project = args.wandb_project\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to load data and createe data loader\n",
    "from data_utils import LanguageModelTensorDataset\n",
    "train_ds_tensor = torch.load('../data/knowledge_graph/knowledge_graph_train_ds.pt')\n",
    "train_ds = LanguageModelTensorDataset(train_ds_tensor)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab\n",
    "n_nodes = 50\n",
    "n_relations = 5\n",
    "\n",
    "node_vocab = [f'N{i}' for i in range(n_nodes)]\n",
    "relation_vocab = [f'R{i}' for i in range(n_relations)]\n",
    "special_tokens = ['<CTX>', '</CTX>', '<SEP>', '<QUERY>', '</QUERY>', '<EOS>', '<PAD>', '<UNK>']\n",
    "vocab = node_vocab + relation_vocab + special_tokens\n",
    "vocab_map = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examples from train__ds_tensor\n",
    "for x in train_ds_tensor:\n",
    "    print(x.tolist())\n",
    "    print(' '.join([vocab[idx] for idx in x]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examples from train_dl\n",
    "train_iter = iter(train_dl)\n",
    "for _ in range(5):\n",
    "    # get next from train_dl\n",
    "    x, y = next(train_iter)\n",
    "    x, y = x.squeeze(), y.squeeze()\n",
    "    print('INPUT')\n",
    "    print(x.tolist())\n",
    "    print(' '.join([vocab[idx] for idx in x]))\n",
    "    print('TARGET')\n",
    "    print(y.tolist())\n",
    "    print(' '.join([vocab[idx] for idx in y]))\n",
    "    print('-'*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Val and Test Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {vocab: i for i, vocab in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "print('vocab size: ', vocab_size) \n",
    "seq_len = len(train_ds_tensor[0])\n",
    "print('seq_len: ', seq_len)\n",
    "\n",
    "# load data and createe data loader\n",
    "\n",
    "# training set\n",
    "from data_utils import LanguageModelTensorDataset\n",
    "train_ds_tensor = torch.load('../data/knowledge_graph/knowledge_graph_train_ds.pt')\n",
    "train_ds = LanguageModelTensorDataset(train_ds_tensor)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=7)\n",
    "\n",
    "\n",
    "# pretraining set\n",
    "train_ds_fact_tensor = torch.load('../data/knowledge_graph/knowledge_graph_train_ds_fact.pt')\n",
    "train_ds_fact = LanguageModelTensorDataset(train_ds_fact_tensor)\n",
    "train_dl_fact = torch.utils.data.DataLoader(train_ds_fact, batch_size=args.batch_size, shuffle=True, num_workers=7)\n",
    "train_dl_fact_test_x = train_ds_fact_tensor[:,:5]\n",
    "train_dl_fact_test_y = train_ds_fact_tensor[:,5:]\n",
    "train_ds_fact_test = torch.utils.data.TensorDataset(train_dl_fact_test_x, train_dl_fact_test_y)\n",
    "train_dl_fact_test = torch.utils.data.DataLoader(train_ds_fact_test, batch_size=args.batch_size, shuffle=True, num_workers=7)\n",
    "\n",
    "test_ds_tensor_control_1 = torch.load('../data/knowledge_graph/knowledge_graph_val_ds_controllability_1.pt')\n",
    "test_ds_control_1 = LanguageModelTensorDataset(test_ds_tensor_control_1)\n",
    "test_dl_control_1_val = torch.utils.data.DataLoader(test_ds_control_1, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "test_ds_control_1_test_x = test_ds_tensor_control_1[:,:9]\n",
    "test_ds_control_1_test_y = test_ds_tensor_control_1[:,9:]\n",
    "test_ds_control_1_test = torch.utils.data.TensorDataset(test_ds_control_1_test_x, test_ds_control_1_test_y)\n",
    "test_dl_control_1 = torch.utils.data.DataLoader(test_ds_control_1_test, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "\n",
    "test_ds_tensor_control_2 = torch.load('../data/knowledge_graph/knowledge_graph_val_ds_controllability_2.pt')\n",
    "test_ds_control_2 = LanguageModelTensorDataset(test_ds_tensor_control_2)\n",
    "test_dl_control_2_val = torch.utils.data.DataLoader(test_ds_control_2, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "test_ds_control_2_test_x = test_ds_tensor_control_2[:,:13]\n",
    "test_ds_control_2_test_y = test_ds_tensor_control_2[:,13:]\n",
    "test_ds_control_2_test = torch.utils.data.TensorDataset(test_ds_control_2_test_x, test_ds_control_2_test_y)\n",
    "test_dl_control_2 = torch.utils.data.DataLoader(test_ds_control_2_test, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "\n",
    "\n",
    "test_ds_tensor_control_3 = torch.load('../data/knowledge_graph/knowledge_graph_val_ds_controllability_3.pt')\n",
    "test_ds_control_3 = LanguageModelTensorDataset(test_ds_tensor_control_3)\n",
    "test_dl_control_3_val = torch.utils.data.DataLoader(test_ds_control_3, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "test_ds_control_3_test_x = test_ds_tensor_control_3[:,:17]\n",
    "test_ds_control_3_test_y = test_ds_tensor_control_3[:,17:]\n",
    "test_ds_control_3_test = torch.utils.data.TensorDataset(test_ds_control_3_test_x, test_ds_control_3_test_y)\n",
    "test_dl_control_3 = torch.utils.data.DataLoader(test_ds_control_3_test, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "\n",
    "\n",
    "test_ds_tensor_robust_1 = torch.load('../data/knowledge_graph/knowledge_graph_val_ds_robustness_1.pt')\n",
    "test_ds_robust_1 = LanguageModelTensorDataset(test_ds_tensor_robust_1)\n",
    "test_dl_robust_1_val = torch.utils.data.DataLoader(test_ds_robust_1, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "test_ds_robust_1_test_x = test_ds_tensor_robust_1[:,:9]\n",
    "test_ds_robust_1_test_y = test_ds_tensor_robust_1[:,9:]\n",
    "test_ds_robust_1_test = torch.utils.data.TensorDataset(test_ds_robust_1_test_x, test_ds_robust_1_test_y)\n",
    "test_dl_robust_1 = torch.utils.data.DataLoader(test_ds_robust_1_test, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "\n",
    "\n",
    "test_ds_tensor_robust_2 = torch.load('../data/knowledge_graph/knowledge_graph_val_ds_robustness_2.pt')\n",
    "test_ds_robust_2 = LanguageModelTensorDataset(test_ds_tensor_robust_2)\n",
    "test_dl_robust_2_val = torch.utils.data.DataLoader(test_ds_robust_2, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "test_ds_robust_2_test_x = test_ds_tensor_robust_2[:,:13]\n",
    "test_ds_robust_2_test_y = test_ds_tensor_robust_2[:,13:]\n",
    "test_ds_robust_2_test = torch.utils.data.TensorDataset(test_ds_robust_2_test_x, test_ds_robust_2_test_y)\n",
    "test_dl_robust_2 = torch.utils.data.DataLoader(test_ds_robust_2_test, batch_size=args.batch_size, shuffle=False, num_workers=7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_ds_tensor_robust_3 = torch.load('../data/knowledge_graph/knowledge_graph_val_ds_robustness_3.pt')\n",
    "test_ds_robust_3 = LanguageModelTensorDataset(test_ds_tensor_robust_3)\n",
    "test_dl_robust_3_val = torch.utils.data.DataLoader(test_ds_robust_3, batch_size=args.batch_size, shuffle=False)\n",
    "test_ds_robust_3_test_x = test_ds_tensor_robust_3[:,:17]\n",
    "test_ds_robust_3_test_y = test_ds_tensor_robust_3[:,17:]\n",
    "test_ds_robust_3_test = torch.utils.data.TensorDataset(test_ds_robust_3_test_x, test_ds_robust_3_test_y)\n",
    "test_dl_robust_3 = torch.utils.data.DataLoader(test_ds_robust_3_test, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# multi validation dataloaders\n",
    "validation_dataloaders = [test_dl_control_1, test_dl_control_2, test_dl_control_3, test_dl_robust_1, test_dl_robust_2, test_dl_robust_3] # NOTE: put this up here instead\n",
    "validation_dataloaders_labels = ['control_1', 'control_2', 'control_3', 'robust_1', 'robust_2', 'robust_3'] # NOTE: added labels for logging\n",
    "\n",
    "print(\"Number of batches: \", len(train_dl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class LitLanguageModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_dl = train_dl\n",
    "        self.mismatched_sequences = []\n",
    "        self.all_seq_acc = []\n",
    "        self.all_per_pos_acc = []\n",
    "        self.facts_loader = train_dl_fact_test\n",
    "        self.truth = []\n",
    "        self.val_pred_mistakes_reason_batch = defaultdict(lambda: defaultdict(list))\n",
    "        self.val_pred_mistakes_reason = defaultdict(lambda: defaultdict(list))\n",
    "        # store (sub,rel,obj) tuples from the truth in the training set\n",
    "        self.truth_extract()\n",
    "        self.context_tuples_mixture_train = self.context_extract(self.train_dl)\n",
    "        self.query_tuples_mixture_train = self.query_extract(self.train_dl)\n",
    "        self.reasons = [\"own_context_ro\", \"other_context_train_dl_sro\", \"query_sro\", \"query_ro\", 'fact']\n",
    "        # TODO: ^ adjust this to only consider the interesting reasons we talked about\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "        # accuracy. torch.where(y==<QUERY>) and find relevant pred_idx accordingly (index add 3)\n",
    "        query_idx = vocab_map['<QUERY>']\n",
    "        pred_idx = torch.where(y[0]==query_idx)[0] + 3\n",
    "        acc = torch.mean((pred[:,pred_idx] == y[:,pred_idx]).float())\n",
    "\n",
    "        self.log('loss/train', loss, prog_bar=True, logger=True, on_step=args.log_on_step, on_epoch=True)\n",
    "        self.log('obj_acc/train', acc, prog_bar=True, logger=True, on_step=args.log_on_step, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # x, y = batch\n",
    "        # # with ctx:\n",
    "        # logits, loss = self.model(x, y)\n",
    "\n",
    "\n",
    "        x, y = batch\n",
    "        # Generate predictions using the generate function, return the entire sequence\n",
    "        predicted_idx = self.model.generate(x, max_new_tokens=y.size(1), top_k=1)\n",
    "        generated_seq = predicted_idx[:, x.size(1):]\n",
    "\n",
    "        seq_acc = self.sequence_accuracy(generated_seq, y)\n",
    "        self.log(f'seq_acc/{validation_dataloaders_labels[dataloader_idx]}', seq_acc) # NOTE\n",
    "\n",
    "        self.capture_mismatches(generated_seq,y, x)\n",
    "        # val_dataloader = self.trainer.val_dataloaders[dataloader_idx]\n",
    "\n",
    "        fact_affected_prediction = self.fact_affected_prediction()\n",
    "        other_reasons = self.context_affected_prediction()\n",
    "\n",
    "        if fact_affected_prediction != None:\n",
    "            # non empty mismatched seq\n",
    "            self.val_pred_mistakes_reason_batch[dataloader_idx]['fact'].append(fact_affected_prediction)\n",
    "            self.log(f'mistake_analysis/{validation_dataloaders_labels[dataloader_idx]}/fact', fact_affected_prediction, add_dataloader_idx=False, prog_bar=True) # NOTE\n",
    "        if other_reasons != None:\n",
    "            for i in range(len(other_reasons)):\n",
    "                self.val_pred_mistakes_reason_batch[dataloader_idx][self.reasons[i]].append(other_reasons[i])\n",
    "                self.log(f'mistake_analysis/{validation_dataloaders_labels[dataloader_idx]}/{self.reasons[i]}', other_reasons[i], add_dataloader_idx=False, prog_bar=True) # NOTE\n",
    "        self.mismatched_sequences.clear()\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "\n",
    "        x, y = batch\n",
    "        # Generate predictions using the generate function, return the entire sequence\n",
    "        predicted_idx = self.model.generate(x, max_new_tokens=y.size(1), top_k=1)\n",
    "        generated_seq = predicted_idx[:, x.size(1):]\n",
    "\n",
    "\n",
    "        # Calculate per position accuracy\n",
    "        per_pos_acc = self.per_position_accuracy(generated_seq, y)\n",
    "\n",
    "        # Calculate whole sequence accuracy\n",
    "        seq_acc = self.sequence_accuracy(generated_seq, y)\n",
    "\n",
    "        # Store metrics for later averaging\n",
    "        self.all_seq_acc.append(seq_acc)\n",
    "        self.all_per_pos_acc.append(per_pos_acc)\n",
    "\n",
    "        self.capture_mismatches(generated_seq,y, x)\n",
    "\n",
    "    def sequence_accuracy(self, preds, targets):\n",
    "        # Check if entire sequences match\n",
    "        correct = torch.all(preds == targets, dim=1)\n",
    "        accuracy = torch.mean(correct.float())\n",
    "        return accuracy\n",
    "\n",
    "    def per_position_accuracy(self, preds, targets):\n",
    "        correct = preds == targets\n",
    "        per_position_acc = correct.float().mean(dim=0)\n",
    "        return per_position_acc\n",
    "\n",
    "    def capture_mismatches(self, preds, targets, inputs, max_examples=1000):\n",
    "        \"\"\"\n",
    "        Store examples of sequences where the predictions do not match the targets.\n",
    "\n",
    "        Args:\n",
    "            preds (torch.Tensor): The model's predicted sequences.\n",
    "            targets (torch.Tensor): The actual sequences.\n",
    "            inputs (torch.Tensor): The input sequences to the model.\n",
    "            max_examples (int): Maximum number of mismatched sequences to store.\n",
    "        \"\"\"\n",
    "        # Convert predictions to class indices\n",
    "        mismatches = preds != targets\n",
    "\n",
    "        # Iterate over the batch to find mismatches\n",
    "        for i in range(mismatches.size(0)):\n",
    "            if torch.any(mismatches[i]) and len(self.mismatched_sequences) < max_examples:\n",
    "                mismatch_data = {\n",
    "                    'input': inputs[i].cpu().numpy(),\n",
    "                    'predicted': preds[i].cpu().numpy(),\n",
    "                    'target': targets[i].cpu().numpy()\n",
    "                }\n",
    "                self.mismatched_sequences.append(mismatch_data)\n",
    "\n",
    "\n",
    "\n",
    "    def truth_extract(self):\n",
    "        \"\"\"\n",
    "        Extracts the truth values for the given object and relation in testing sets.\n",
    "        \"\"\"\n",
    "\n",
    "        for i, batch_i_data in enumerate(self.facts_loader):\n",
    "            x,y = batch_i_data\n",
    "            for j in range(x.size(0)):\n",
    "                sub = x[j,-2]\n",
    "                rel = x[j,-1]\n",
    "                obj = y[j,0]\n",
    "                self.truth.append((sub, rel, obj))\n",
    "\n",
    "\n",
    "    def fact_affected_prediction(self):\n",
    "        if not self.mismatched_sequences:\n",
    "            return None\n",
    "        counter = 0\n",
    "        for mismatch in self.mismatched_sequences:\n",
    "            input_seq = mismatch['input']\n",
    "            predicted_seq = mismatch['predicted']\n",
    "            target_seq = mismatch['target']\n",
    "\n",
    "            # find the object and relation in the input sequence\n",
    "\n",
    "            for i in range(len(input_seq)):\n",
    "                # print(i, input_seq[i])\n",
    "                if input_seq[i] == vocab_map['<QUERY>']:\n",
    "                    sub = input_seq[i+1]\n",
    "                    rel = input_seq[i+2]\n",
    "                    obj_target = target_seq[0]\n",
    "                    obj_pred = predicted_seq[0]\n",
    "\n",
    "                    if (sub, rel, obj_pred) in self.truth:\n",
    "                        counter += 1\n",
    "                    break\n",
    "\n",
    "        return counter/len(self.mismatched_sequences)\n",
    "\n",
    "    def context_extract(self, dataloader):\n",
    "        context_tuples_mixture_train =[]\n",
    "        for x, y in dataloader:\n",
    "            for i in range(x.size(0)):\n",
    "                for j in range(x.size(1)):\n",
    "                    if x[i,j] == vocab_map['<CTX>']:\n",
    "                        context_tuples_mixture_train.append(x[i,j+1: j+4])\n",
    "                    elif x[i,j] == vocab_map['<SEP>'] and x[i,j+1] != vocab_map['</CTX>']:\n",
    "                        context_tuples_mixture_train.append(x[i,j+1: j+4])\n",
    "                    elif x[i,j] == vocab_map['</CTX>']:\n",
    "                        break\n",
    "        return context_tuples_mixture_train\n",
    "\n",
    "\n",
    "    def query_extract(self, dataloader):\n",
    "        query_tuples_mixture_train =[]\n",
    "        for x, y in dataloader:\n",
    "            for i in range(x.size(0)):\n",
    "                for j in range(x.size(1)):\n",
    "                    if x[i,j] == vocab_map['<QUERY>']:\n",
    "                        query_tuples_mixture_train.append(x[i,j+1: j+4])\n",
    "                        break\n",
    "        return query_tuples_mixture_train\n",
    "\n",
    "\n",
    "\n",
    "    def context_affected_prediction(self):\n",
    "\n",
    "        if not self.mismatched_sequences:\n",
    "            return None\n",
    "        counter_own_context = 0\n",
    "        counter_other_context = 0\n",
    "        counter_query = 0\n",
    "        counter_query_ro = 0\n",
    "        for mismatch in self.mismatched_sequences:\n",
    "            input_seq = mismatch['input']\n",
    "            predicted_seq = mismatch['predicted']\n",
    "            target_seq = mismatch['target']\n",
    "\n",
    "            # find the object and relation in the input sequence\n",
    "            context_examples = []\n",
    "            for i in range(len(input_seq)):\n",
    "                # organize examples from context\n",
    "                if input_seq[i] == vocab_map['<CTX>']:\n",
    "                    context_examples.append(input_seq[i+1:i+4])\n",
    "                elif input_seq[i] == vocab_map['<SEP>'] and input_seq[i] != vocab_map['</CTX>']:\n",
    "                    context_examples.append(input_seq[i+1:i+4])\n",
    "\n",
    "                # TODO: remove the parts that are not needed anymore (e.g., checking against training set)\n",
    "                elif input_seq[i] == vocab_map['<QUERY>']:\n",
    "                    sub = input_seq[i+1]\n",
    "                    rel = input_seq[i+2]\n",
    "                    obj_target = target_seq[0]\n",
    "                    obj_pred = predicted_seq[0]\n",
    "                    if (sub, rel, obj_pred) in self.context_tuples_mixture_train:\n",
    "                        #print(\"affected by some context in mixture training\",sub, rel, obj_pred)\n",
    "                        counter_other_context+=1\n",
    "\n",
    "                    for (s,r,o) in context_examples:\n",
    "                        if (r == rel) and (o == obj_pred):\n",
    "                        #print(\"affected by its own context\",input_seq)\n",
    "                            counter_own_context+=1\n",
    "                            break\n",
    "\n",
    "                    if (sub, rel, obj_pred) in self.query_tuples_mixture_train:\n",
    "                        counter_query+=1\n",
    "\n",
    "                    for (s,r,o) in self.query_tuples_mixture_train:\n",
    "                        if (r == rel) and (o == obj_pred):\n",
    "                            counter_query_ro +=1\n",
    "                            break\n",
    "                    break\n",
    "            num_mistakes = len(self.mismatched_sequences)\n",
    "        # return counter_own_context/num_mistakes, counter_other_context/num_mistakes, counter_query/num_mistakes, counter_query_ro/num_mistakes\n",
    "        return counter_own_context/num_mistakes, counter_other_context/num_mistakes, counter_query/num_mistakes, counter_query_ro/num_mistakes\n",
    "\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Calculate overall metrics from all batches\n",
    "        if self.all_seq_acc and self.all_per_pos_acc:\n",
    "\n",
    "            overall_seq_acc = torch.mean(torch.stack(self.all_seq_acc))\n",
    "            overall_per_pos_acc = torch.mean(torch.stack(self.all_per_pos_acc), dim=0)\n",
    "            # Log overall metrics\n",
    "            self.log('test_overall_seq_acc', overall_seq_acc, on_step=False, on_epoch=True)\n",
    "            self.log('test_overall_per_pos_acc_mean', overall_per_pos_acc.mean(), on_step=False, on_epoch=True)\n",
    "\n",
    "            # Plot overall per position accuracy\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(overall_per_pos_acc.cpu().numpy(), marker='o', linestyle='-')\n",
    "            plt.title('Overall Per Position Accuracy')\n",
    "            plt.xlabel('Position')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "            # Clear the accumulators for the next epoch\n",
    "            self.all_seq_acc = []\n",
    "            self.all_per_pos_acc = []\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Calculate overall metrics from all batches\n",
    "        for dataloader_idx in self.val_pred_mistakes_reason_batch.keys():\n",
    "            for reason in self.reasons:\n",
    "                if self.val_pred_mistakes_reason_batch[dataloader_idx][reason]:\n",
    "                    overall_affected_prediction = torch.mean(torch.tensor(self.val_pred_mistakes_reason_batch[dataloader_idx][reason]))\n",
    "                    self.val_pred_mistakes_reason[dataloader_idx][reason].append(overall_affected_prediction)\n",
    "                    self.val_pred_mistakes_reason_batch[dataloader_idx][reason].clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # NOTE: i chose these arbitrarily; may need to be tuned\n",
    "        optimizer = configure_optimizers(self.model, weight_decay=0.0, learning_rate=1e-3, betas=(0.9, 0.999), device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = args.d_model # 128\n",
    "dff = args.dff # 256\n",
    "n_layers = args.n_layers\n",
    "n_heads = args.n_heads\n",
    "dropout_rate = 0.1\n",
    "activation = 'gelu'\n",
    "norm_first = True\n",
    "\n",
    "\n",
    "model_args = dict(\n",
    "    vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_heads=n_heads, dff=dff,\n",
    "    dropout_rate=dropout_rate, activation=activation, norm_first=norm_first, max_block_size=seq_len)\n",
    "model = TransformerLM(**model_args)#.to(device)\n",
    "torchinfo.summary(model, row_settings=[\"depth\", \"var_names\"], col_names=[\"num_params\", \"params_percent\", \"trainable\"], depth=3, col_width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitLanguageModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = args.n_epochs\n",
    "\n",
    "# wandb_logger = WandbLogger(experiment=run, log_model=False) # name=run_name, project=wandb_project,\n",
    "# wandb_logger.watch(model, log_graph=False)\n",
    "# wandb_logger = None\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.TQDMProgressBar(refresh_rate=50)\n",
    "    # L.pytorch.callbacks.RichProgressBar()\n",
    "]\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=n_epochs, enable_checkpointing=True, enable_model_summary=True, #precision='64-true',\n",
    "    callbacks=callbacks, enable_progress_bar=True, check_val_every_n_epoch=1, # limit_train_batches=200, limit_val_batches=200,\n",
    "    logger=False\n",
    "    )\n",
    "\n",
    "with ctx:\n",
    "    trainer.fit(model=lit_model, train_dataloaders=train_dl_fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set mistake reasons\n",
    "print(lit_model.val_pred_mistakes_reason)\n",
    "pred_mistakes_fact_reason_train_w_fact_only = lit_model.val_pred_mistakes_reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y) in train_dl_fact:\n",
    "    query = x[:, :5]\n",
    "    ans = x[:, 5]\n",
    "    pred, _ = model(query)\n",
    "    pred_idx = torch.argmax(pred, dim=-1).squeeze()\n",
    "    print(query[0])\n",
    "    print(ans[0])\n",
    "    print(pred_idx[0])\n",
    "    print(x[0][:8])\n",
    "    acc = torch.mean((ans == pred_idx).float())\n",
    "    print(acc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on pure fact, make sure it achieves 100% accuracy\n",
    "trainer.test(model=lit_model, dataloaders=train_dl_fact_test)\n",
    "reverse_vocab_map = {idx: vocab for vocab, idx in vocab_map.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_1)\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_2)\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_3)\n",
    "lit_model.mismatched_sequences.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_1)\n",
    "\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_2)\n",
    "\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_3)\n",
    "lit_model.mismatched_sequences.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"finetune\" on mixture dataset: 1/3 fact 1/3 counterfactual 1/3 robustness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune on mixture dataset: 1/3 fact 1/3 counterfactual 1/3 robustness\n",
    "\n",
    "\n",
    "wandb_project = 'controllability_robustness' # NOTE\n",
    "group_name = None # NOTE\n",
    "run_name =  datetime.now().strftime(\"%Y-%m-%d--%H:%M:%S\")# NOTE\n",
    "run = wandb.init(project=wandb_project, group=group_name, name=run_name,\n",
    "    config={'group': group_name, **model_args}) # NOTE\n",
    "\n",
    "wandb_logger = WandbLogger(experiment=run, log_model=False) # name=run_name, project=wandb_project,\n",
    "# wandb_logger.watch(model, log_graph=False)\n",
    "# wandb_logger = None\n",
    "logger = wandb_logger if wandb_log else None\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=n_epochs, enable_checkpointing=True, enable_model_summary=True, #precision='64-true',\n",
    "    callbacks=callbacks, enable_progress_bar=True, check_val_every_n_epoch=1, # limit_train_batches=200, limit_val_batches=200,\n",
    "    logger=logger,\n",
    "    )\n",
    "\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dl, val_dataloaders=validation_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab_map = {i: word for word, i in vocab_map.items()}\n",
    "\n",
    "# model performance on the pure fact pretraining set\n",
    "trainer.test(model=lit_model, dataloaders=train_dl_fact_test)\n",
    "# model performance on the validation set\n",
    "lit_model.mismatched_sequences.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set mistake reasons\n",
    "print(lit_model.val_pred_mistakes_reason)\n",
    "pred_mistakes_fact_reason_train_w_mxiture = lit_model.val_pred_mistakes_reason\n",
    "# Plot each series with a different color\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  \n",
    "reasons = pred_mistakes_fact_reason_train_w_mxiture[0].keys()\n",
    "for reason in reasons:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for idx, (key, values) in enumerate(pred_mistakes_fact_reason_train_w_mxiture.items()):\n",
    "        if key <=2:\n",
    "            dataname = f'Controllability {key+1}'\n",
    "        else:\n",
    "            dataname = f'Robustness {key-2}'\n",
    "        plt.plot(values[reason], label=dataname, color=colors[idx % len(colors)])\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.title(f'Proportion of mistakes affected by {reason}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on counterfactual context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(seq, reverse_vocab_map):\n",
    "    return ' '.join([reverse_vocab_map[idx] for idx in seq])\n",
    "\n",
    "def mismatch_seq_print(mismatch, reverse_vocab_map, max_examples=3):\n",
    "    for mismatch in lit_model.mismatched_sequences[:max_examples]:\n",
    "        input_seq = decode_sequence(mismatch['input'], reverse_vocab_map)\n",
    "        predicted_seq = decode_sequence(mismatch['predicted'], reverse_vocab_map)\n",
    "        target_seq = decode_sequence(mismatch['target'], reverse_vocab_map)\n",
    "        \n",
    "        print(f\"Input: {input_seq}\")\n",
    "        print(f\"Predicted: {predicted_seq}\")\n",
    "        print(f\"Target: {target_seq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_1)\n",
    "mismatch_seq_print(lit_model.mismatched_sequences, reverse_vocab_map)\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_2)\n",
    "mismatch_seq_print(lit_model.mismatched_sequences, reverse_vocab_map)\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_3)\n",
    "mismatch_seq_print(lit_model.mismatched_sequences, reverse_vocab_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Robustness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_1)\n",
    "mismatch_seq_print(lit_model.mismatched_sequences, reverse_vocab_map,max_examples=6)\n",
    "\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_2)\n",
    "mismatch_seq_print(lit_model.mismatched_sequences, reverse_vocab_map, max_examples=6)\n",
    "\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_3)\n",
    "mismatch_seq_print(lit_model.mismatched_sequences, reverse_vocab_map, max_examples=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_extract(dataloader):\n",
    "    context_tuples_mixture_train =[]\n",
    "    for x, y in dataloader:\n",
    "        for i in range(x.size(0)):\n",
    "            for j in range(x.size(1)):\n",
    "                if x[i,j] == vocab_map['<CTX>']:\n",
    "                    context_tuples_mixture_train.append(x[i,j+1: j+4])\n",
    "                elif x[i,j] == vocab_map['<SEP>'] and x[i,j+1] != vocab_map['</CTX>']:    \n",
    "                    context_tuples_mixture_train.append(x[i,j+1: j+4])\n",
    "                elif x[i,j] == vocab_map['</CTX>']:\n",
    "                    break\n",
    "    return context_tuples_mixture_train\n",
    "\n",
    "def query_extract(dataloader):\n",
    "    query_tuples_mixture_train =[]\n",
    "    for x, y in dataloader:\n",
    "        for i in range(x.size(0)):\n",
    "            for j in range(x.size(1)):\n",
    "                if x[i,j] == vocab_map['<QUERY>']:\n",
    "                    query_tuples_mixture_train.append(x[i,j+1: j+4])\n",
    "                    break\n",
    "    return query_tuples_mixture_train\n",
    "\n",
    "    \n",
    "#print(context_extract(train_dl))\n",
    "\n",
    "def context_affected_prediction(lit_model,context_tuples_mixture_train, query_tuples_mixture_train):\n",
    "    \n",
    "    if not lit_model.mismatched_sequences:\n",
    "        return None\n",
    "    counter_own_context = 0\n",
    "    counter_other_context = 0\n",
    "    counter_query = 0\n",
    "    counter_query_ro = 0\n",
    "    for mismatch in lit_model.mismatched_sequences:\n",
    "        input_seq = mismatch['input']\n",
    "        predicted_seq = mismatch['predicted']\n",
    "        target_seq = mismatch['target']\n",
    "        \n",
    "        # find the object and relation in the input sequence\n",
    "        context_examples = []\n",
    "        for i in range(len(input_seq)):\n",
    "            # organize examples from context\n",
    "            if input_seq[i] == vocab_map['<CTX>']:\n",
    "                context_examples.append(input_seq[i+1:i+4])\n",
    "            elif input_seq[i] == vocab_map['<SEP>'] and input_seq[i] != vocab_map['</CTX>']:\n",
    "                context_examples.append(input_seq[i+1:i+4])\n",
    "                \n",
    "\n",
    "\n",
    "            elif input_seq[i] == vocab_map['<QUERY>']:\n",
    "                sub = input_seq[i+1]\n",
    "                rel = input_seq[i+2]\n",
    "                obj_target = target_seq[0]\n",
    "                obj_pred = predicted_seq[0]\n",
    "                if (sub, rel, obj_pred) in context_tuples_mixture_train:\n",
    "                    #print(\"affected by some context in mixture training\",sub, rel, obj_pred)\n",
    "                    counter_other_context+=1\n",
    "                \n",
    "                for (s,r,o) in context_examples:\n",
    "                    if (r == rel) and (o == obj_pred):\n",
    "                    #print(\"affected by its own context\",input_seq)   \n",
    "                        counter_own_context+=1\n",
    "                        break\n",
    "\n",
    "                if (sub, rel, obj_pred) in query_tuples_mixture_train:\n",
    "                    counter_query+=1\n",
    "\n",
    "                for (s,r,o) in query_tuples_mixture_train:\n",
    "                    if (r == rel) and (o == obj_pred):\n",
    "                        counter_query_ro +=1\n",
    "                        break\n",
    "                break\n",
    "        num_mistakes = len(lit_model.mismatched_sequences)\n",
    "    return counter_own_context/num_mistakes, counter_other_context/num_mistakes, counter_query/num_mistakes, counter_query_ro/num_mistakes\n",
    "\n",
    "context_tuples_mixture_train = context_extract(train_dl)\n",
    "query_tuples_mixture_train = query_extract(train_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_1)\n",
    "print(\"own_context_ro\", \"other_context_train_dl_sro\", \"train_dl_query_sro\", \"train_dl_query_ro\", 'fact')\n",
    "print(context_affected_prediction(lit_model, context_tuples_mixture_train, query_tuples_mixture_train))\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_2)\n",
    "print(\"own_context_ro\", \"other_context_train_dl_sro\", \"train_dl_query_sro\", \"train_dl_query_ro\", 'fact')\n",
    "print(context_affected_prediction(lit_model, context_tuples_mixture_train, query_tuples_mixture_train))\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_control_3)\n",
    "print(\"own_context_ro\", \"other_context_train_dl_sro\", \"train_dl_query_sro\", \"train_dl_query_ro\", 'fact')\n",
    "print(context_affected_prediction(lit_model, context_tuples_mixture_train, query_tuples_mixture_train))\n",
    "\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_1)\n",
    "print(\"own_context_ro\", \"other_context_train_dl_sro\", \"train_dl_query_sro\", \"train_dl_query_ro\", 'fact')\n",
    "print(context_affected_prediction(lit_model, context_tuples_mixture_train, query_tuples_mixture_train))\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_2)\n",
    "print(\"own_context_ro\", \"other_context_train_dl_sro\", \"train_dl_query_sro\", \"train_dl_query_ro\", 'fact')\n",
    "print(context_affected_prediction(lit_model, context_tuples_mixture_train, query_tuples_mixture_train))\n",
    "lit_model.mismatched_sequences.clear()\n",
    "trainer.test(model=lit_model, dataloaders=test_dl_robust_3)\n",
    "print(\"own_context_ro\", \"other_context_train_dl_sro\", \"train_dl_query_sro\", \"train_dl_query_ro\", 'fact')\n",
    "print(context_affected_prediction(lit_model, context_tuples_mixture_train, query_tuples_mixture_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstract_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
